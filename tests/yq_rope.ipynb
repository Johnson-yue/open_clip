{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn RoPe Position Embeddings\n",
    "\n",
    "## 4 github\n",
    "* 1) https://github.com/aju22/RoPE-PyTorch?tab=readme-ov-file (failed)\n",
    "* 2) https://github.com/srishti-git1110/RoPE-PyTorch (failed)\n",
    "* 3) https://github.com/JunnYu/RoFormer_pytorch/blob/roformer_v2/src/roformer/modeling_roformer.py#L156\n",
    "* 4) (lucidrains-rotary-embedding-torch):https://github.com/lucidrains/rotary-embedding-torch \n",
    "* 5) (LLama):https://github.com/huggingface/transformers/blob/3a8eb74668e9c2cc563b2f5c62fac174797063e0/src/transformers/models/llama/modeling_llama.py#L203\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "q = torch.randn(1, 8, 5, 10)  # (batch, heads, seq_len, seq_dim)\n",
    "k = torch.randn(1, 8, 5, 10)  # (batch, heads, seq_len, seq_dim) \n",
    "\n",
    "q = torch.randn(1, 5, 10)  # (batch, heads, seq_len, seq_dim)\n",
    "k = torch.randn(1, 5, 10)  # (batch, heads, seq_len, seq_dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## git-1 is not work and git-2 not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)[RoPE-Pytorch] colab: https://colab.research.google.com/drive/11SKfzvMotuvvXNqY9qBpsD2RQX1PK7rP?usp=sharing#scrollTo=TFBGxdQef_WF\n",
    "\n",
    "# This Implement is Wrong\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "\n",
    "  def __init__(self, d: int, base: int = 10_000):\n",
    "\n",
    "    super().__init__()\n",
    "    self.base = base\n",
    "    self.d = d\n",
    "    self.cos_cached = None\n",
    "    self.sin_cached = None\n",
    "\n",
    "  def _build_cache(self, x: torch.Tensor):\n",
    "\n",
    "    if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "      return\n",
    "\n",
    "    seq_len = x.shape[0]\n",
    "\n",
    "    theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device) # THETA = 10,000^(-2*i/d) or 1/10,000^(2i/d), i~(0, d/2)\n",
    "\n",
    "    seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device) #Position Index -> [0,1,2...seq-1]\n",
    "\n",
    "    idx_theta = torch.einsum('n,d->nd', seq_idx, theta)  #Calculates m*(THETA) = [ [0, 0...], [THETA_1, THETA_2...THETA_d/2], ... [seq-1*(THETA_1), seq-1*(THETA_2)...] ]\n",
    "\n",
    "    idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1) # [THETA_1, THETA_2...THETA_d/2] -> [THETA_1, THETA_2...THETA_d]\n",
    "\n",
    "\n",
    "    self.cos_cached = idx_theta2.cos()[:, None, None, :] #Cache [cosTHETA_1, cosTHETA_2...cosTHETA_d]\n",
    "    self.sin_cached = idx_theta2.sin()[:, None, None, :] #cache [sinTHETA_1, sinTHETA_2...sinTHETA_d]\n",
    "\n",
    "  def _neg_half(self, x: torch.Tensor):\n",
    "\n",
    "    d_2 = self.d // 2 #\n",
    "\n",
    "    return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1) # [x_1, x_2,...x_d] -> [-x_d/2, ... -x_d, x_1, ... x_d/2]\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "\n",
    "    self._build_cache(x)\n",
    "\n",
    "    neg_half_x = self._neg_half(x)\n",
    "\n",
    "    x_rope = (x * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]]) # [x_1*cosTHETA_1 - x_d/2*sinTHETA_d/2, ....]\n",
    "\n",
    "    return x_rope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "neg_out:\n",
      " tensor([[-4, -5, -6, -7,  0,  1,  2,  3]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yue/anaconda3/envs/font/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucidrain out\n",
      " tensor([[-1,  0, -3,  2, -5,  4, -7,  6]])\n"
     ]
    }
   ],
   "source": [
    "def neg_half( x: torch.Tensor):\n",
    "    # It is wrong\n",
    "    d_2 = x.size(-1) // 2 #\n",
    "\n",
    "    return torch.cat([-x[..., d_2:], x[..., :d_2]], dim=-1) # [x_1, x_2,...x_d] -> [-x_d/2, ... -x_d, x_1, ... x_d/2]\n",
    "\n",
    "x = torch.arange(0,8).unsqueeze(0)\n",
    "print(\"x=\\n\",x)\n",
    "neg_half_out = neg_half(x)\n",
    "print(\"neg_out:\\n\", neg_half_out)\n",
    "\n",
    "def lucidrain_rotate_half(x:torch.Tensor):\n",
    "    x = einops.rearrange(x, '... (d r) -> ... d r', r=2)\n",
    "    x1, x2 = x.unbind(dim=-1)\n",
    "    x = torch.stack((-x2, x1), dim=-1)\n",
    "    return einops.rearrange(x, '... d r -> ... (d r)')\n",
    "\n",
    "print(\"lucidrain out\\n\", lucidrain_rotate_half(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## git-2 Roformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roformer_apply_rotary out\n",
      " tensor([ 0, -1,  1,  0,  2, -3,  3,  2,  4, -5,  5,  4,  6, -7,  7,  6])\n"
     ]
    }
   ],
   "source": [
    "# Roformer-pytorch :apply_rotary function test \n",
    "# this is right\n",
    "\n",
    "def roformer_apply_rotary(x:torch.Tensor):\n",
    "    # x.dim = 1  ,like [0, 1, 2,3, 4, 5, 6, 7]\n",
    "    x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "\n",
    "    # x1 * cos - x2 * sin, x2 * cos + x1 * sin\n",
    "    return torch.stack([x1 , -x2 , x2 , x1 ], dim=-1).flatten(-2, -1)\n",
    "    \n",
    "print(\"roformer_apply_rotary out\\n\", roformer_apply_rotary(x.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sinusoidal pe:\n",
      " torch.Size([1, 1, 5, 10])\n",
      "m=0 tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "m=1 tensor([8.4147e-01, 5.4030e-01, 1.5783e-01, 9.8747e-01, 2.5116e-02, 9.9968e-01,\n",
      "        3.9811e-03, 9.9999e-01, 6.3096e-04, 1.0000e+00])\n",
      "\n",
      "\n",
      "\n",
      "original q=\n",
      " tensor([[[ 1.1038, -1.3706, -1.8596, -2.8053, -0.2778,  0.6380,  0.8012,\n",
      "          -0.5019, -0.4612,  1.3554],\n",
      "         [ 0.3524,  0.3652, -0.0210, -0.8388,  0.0778,  0.7852, -1.3597,\n",
      "           1.0027, -0.4950,  0.6346],\n",
      "         [-1.6956,  1.7192,  0.8985,  0.4210,  0.8509,  0.8137,  0.0378,\n",
      "           0.8861,  0.3078,  0.1393],\n",
      "         [ 0.0644, -0.5496,  1.0841, -1.1425,  0.9703,  0.3511,  1.1185,\n",
      "          -0.5006, -2.1818,  0.7248],\n",
      "         [-0.1937,  0.9974,  1.4355, -0.7194,  0.3976, -0.4861, -1.3617,\n",
      "          -0.9791, -0.7754,  0.6222]]])\n",
      "rotary(git3) q=\n",
      " tensor([[[[ 1.1038, -1.3706, -1.8596, -2.8053, -0.2778,  0.6380,  0.8012,\n",
      "           -0.5019, -0.4612,  1.3554],\n",
      "          [-0.1169,  0.4939,  0.1117, -0.8316,  0.0581,  0.7869, -1.3636,\n",
      "            0.9973, -0.4954,  0.6343],\n",
      "          [-0.8576, -2.2573,  0.7225,  0.6801,  0.8090,  0.8555,  0.0308,\n",
      "            0.8864,  0.3076,  0.1397],\n",
      "          [ 0.0137,  0.5532,  1.4868, -0.5195,  0.9411,  0.4232,  1.1244,\n",
      "           -0.4872, -2.1832,  0.7206],\n",
      "          [ 0.8814, -0.5054,  1.5827,  0.2707,  0.4444, -0.4437, -1.3459,\n",
      "           -1.0007, -0.7770,  0.6202]]]])\n"
     ]
    }
   ],
   "source": [
    "# Copied from transformers.models.marian.modeling_marian.MarianSinusoidalPositionalEmbedding with Marian->RoFormer\n",
    "# This is naive SinusoidalPositionalEmbedding \n",
    "# the position embedding feature is only relative on m, not relative on q\n",
    "import numpy as np\n",
    "\n",
    "class RoFormerSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_positions: int, embedding_dim: int, padding_idx: int = None\n",
    "    ):\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self, self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(self,out: nn.Parameter):\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [\n",
    "                [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "                for pos in range(n_pos)\n",
    "            ]\n",
    "        )\n",
    "        self.position_enc = position_enc\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, seq_len: int, past_key_values_length: int = 0):\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length,\n",
    "            past_key_values_length + seq_len,\n",
    "            dtype=torch.long,\n",
    "            device=self.weight.device,\n",
    "        )\n",
    "        return super().forward(positions)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def apply_rotary(self, x, sinusoidal_pos):\n",
    "        sinusoidal_pos = sinusoidal_pos[None, None, :, :].chunk(2, dim=-1)\n",
    "        sin, cos = sinusoidal_pos\n",
    "\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        # 如果是旋转query key的话，下面这个直接cat就行，因为要进行矩阵乘法，最终会在这个维度求和。（只要保持query和key的最后一个dim的每一个位置对应上就可以）\n",
    "        # torch.cat([x1 * cos - x2 * sin, x2 * cos + x1 * sin], dim=-1)\n",
    "        # 如果是旋转value的话，下面这个stack后再flatten才可以，因为训练好的模型最后一个dim是两两之间交替的。\n",
    "        return torch.stack([x1 * cos - x2 * sin, x2 * cos + x1 * sin], dim=-1).flatten(-2, -1)\n",
    "    \n",
    "# Test positionEmbedding output\n",
    "pe = RoFormerSinusoidalPositionalEmbedding(num_positions=q.size(-2), embedding_dim=q.size(-1))\n",
    "\n",
    "pe_enc = pe.position_enc\n",
    "\n",
    "pe_weight = pe.weight\n",
    "# pe weight format is \n",
    "\"\"\" PE weight format\n",
    "m   -   dim=0 -> d/2        |  d/2+1 -> d \n",
    "0       sin(0*theta_i)...   | cos(0 * theta_i)\n",
    "1       sin(1*theta_i)...   | cos(1 * theta_i)\n",
    "2       sin(2*theta_i)...   | cos(2 * theta_i)\n",
    "3       sin(3*theta_i)...   | cos(3 * theta_i)\n",
    "\n",
    "k       sin(k*theta_i)...   | cos(k * theta_i)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Usage when inference:\n",
    "# P_(m, 2i)  = sin(m*theta_i)\n",
    "# P_(m, 2i+1)= cos(m*theta_i)\n",
    "##[sequence_length, embed_size_per_head] -> sin & cos [batch_size, num_heads, sequence_length, embed_size_per_head // 2]\n",
    "sinusoidal_pos = pe(seq_len=q.size(-2), past_key_values_length=0)[\n",
    "            None, None, :, :\n",
    "        ].chunk(2, dim=-1)\n",
    "\n",
    "# Copied from https://github.com/JunnYu/RoFormer_pytorch/blob/447aa8f6e6ddec28e0cffb06eeb12f2d33fb2724/src/roformer/modeling_roformer.py#L441\n",
    "sin, cos = sinusoidal_pos\n",
    "naive_sinusoidal =torch.stack([sin, cos], dim=-1).flatten(-2, -1)\n",
    "\n",
    "print(\"sinusoidal pe:\\n\", naive_sinusoidal.size())\n",
    "\n",
    "print(\"m=0\", naive_sinusoidal[0][0][0])\n",
    "print(\"m=1\", naive_sinusoidal[0][0][1])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "rotary_git3_q = pe.apply_rotary(q, pe(seq_len=q.size(-2), past_key_values_length=0))\n",
    "print(\"original q=\\n\",q)\n",
    "print(\"rotary(git3) q=\\n\", rotary_git3_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## git-4-lucidrains-rotary-embedding-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 324\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m freqs\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Inference code\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m rotary_git4_emb \u001b[38;5;241m=\u001b[39m RotaryEmbedding(dim \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    325\u001b[0m rotary_git4_q \u001b[38;5;241m=\u001b[39m rotary_git4_emb\u001b[38;5;241m.\u001b[39mrotate_queries_or_keys(q)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary_git4_q=\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, rotary_git4_q)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "# Copied from:https://github.com/lucidrains/rotary-embedding-torch/blob/main/rotary_embedding_torch/rotary_embedding_torch.py\n",
    "from __future__ import annotations\n",
    "from math import pi, log\n",
    "\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch import nn, einsum, broadcast_tensors, Tensor\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# broadcat, as tortoise-tts was using it\n",
    "\n",
    "def broadcat(tensors, dim = -1):\n",
    "    broadcasted_tensors = broadcast_tensors(*tensors)\n",
    "    return torch.cat(broadcasted_tensors, dim = dim)\n",
    "\n",
    "def slice_at_dim(t, dim_slice: slice, *, dim):\n",
    "    dim += (t.ndim if dim < 0 else 0)\n",
    "    colons = [slice(None)] * t.ndim\n",
    "    colons[dim] = dim_slice\n",
    "    return t[tuple(colons)]\n",
    "\n",
    "# rotary embedding helper functions\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n",
    "    x1, x2 = x.unbind(dim = -1)\n",
    "    x = torch.stack((-x2, x1), dim = -1)\n",
    "    return rearrange(x, '... d r -> ... (d r)')\n",
    "\n",
    "@autocast('cuda', enabled = False)\n",
    "def apply_rotary_emb(\n",
    "    freqs,\n",
    "    t,\n",
    "    start_index = 0,\n",
    "    scale = 1.,\n",
    "    seq_dim = -2,\n",
    "    freqs_seq_dim = None\n",
    "):\n",
    "    dtype = t.dtype\n",
    "\n",
    "    if not exists(freqs_seq_dim):\n",
    "        if freqs.ndim == 2 or t.ndim == 3:\n",
    "            freqs_seq_dim = 0\n",
    "\n",
    "    if t.ndim == 3 or exists(freqs_seq_dim):\n",
    "        seq_len = t.shape[seq_dim]\n",
    "        freqs = slice_at_dim(freqs, slice(-seq_len, None), dim = freqs_seq_dim)\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    end_index = start_index + rot_dim\n",
    "\n",
    "    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n",
    "\n",
    "    # Split t into three parts: left, middle (to be transformed), and right\n",
    "    t_left = t[..., :start_index]\n",
    "    t_middle = t[..., start_index:end_index]\n",
    "    t_right = t[..., end_index:]\n",
    "\n",
    "    # Apply rotary embeddings without modifying t in place    \n",
    "    t_transformed = (t_middle * freqs.cos() * scale) + (rotate_half(t_middle) * freqs.sin() * scale)\n",
    "        \n",
    "    out = torch.cat((t_left, t_transformed, t_right), dim=-1)\n",
    "\n",
    "    return out.type(dtype)\n",
    "\n",
    "# learned rotation helpers\n",
    "\n",
    "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
    "    if exists(freq_ranges):\n",
    "        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n",
    "        rotations = rearrange(rotations, '... r f -> ... (r f)')\n",
    "\n",
    "    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n",
    "    return apply_rotary_emb(rotations, t, start_index = start_index)\n",
    "\n",
    "# classes\n",
    "\n",
    "class RotaryEmbedding(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        custom_freqs: Tensor | None = None,\n",
    "        freqs_for:  Literal['lang', 'pixel', 'constant'] = 'lang',\n",
    "        theta = 10000,\n",
    "        max_freq = 10,\n",
    "        num_freqs = 1,\n",
    "        learned_freq = False,\n",
    "        use_xpos = False,\n",
    "        xpos_scale_base = 512,\n",
    "        interpolate_factor = 1.,\n",
    "        theta_rescale_factor = 1.,\n",
    "        seq_before_head_dim = False,\n",
    "        cache_if_possible = True,\n",
    "        cache_max_seq_len = 8192\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n",
    "        # has some connection to NTK literature\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n",
    "\n",
    "        theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
    "\n",
    "        self.freqs_for = freqs_for\n",
    "\n",
    "        if exists(custom_freqs):\n",
    "            freqs = custom_freqs\n",
    "        elif freqs_for == 'lang':\n",
    "            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "        elif freqs_for == 'pixel':\n",
    "            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n",
    "        elif freqs_for == 'constant':\n",
    "            freqs = torch.ones(num_freqs).float()\n",
    "\n",
    "        self.cache_if_possible = cache_if_possible\n",
    "        self.cache_max_seq_len = cache_max_seq_len\n",
    "\n",
    "        self.register_buffer('cached_freqs', torch.zeros(cache_max_seq_len, dim), persistent = False)\n",
    "        self.register_buffer('cached_freqs_seq_len', torch.tensor(0), persistent = False)\n",
    "\n",
    "        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n",
    "\n",
    "        self.learned_freq = learned_freq\n",
    "\n",
    "        # dummy for device\n",
    "\n",
    "        self.register_buffer('dummy', torch.tensor(0), persistent = False)\n",
    "\n",
    "        # default sequence dimension\n",
    "\n",
    "        self.seq_before_head_dim = seq_before_head_dim\n",
    "        self.default_seq_dim = -3 if seq_before_head_dim else -2\n",
    "\n",
    "        # interpolation factors\n",
    "\n",
    "        assert interpolate_factor >= 1.\n",
    "        self.interpolate_factor = interpolate_factor\n",
    "\n",
    "        # xpos\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "\n",
    "        if not use_xpos:\n",
    "            return\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.scale_base = xpos_scale_base\n",
    "\n",
    "        self.register_buffer('scale', scale, persistent = False)\n",
    "        self.register_buffer('cached_scales', torch.zeros(cache_max_seq_len, dim), persistent = False)\n",
    "        self.register_buffer('cached_scales_seq_len', torch.tensor(0), persistent = False)\n",
    "\n",
    "        # add apply_rotary_emb as static method\n",
    "\n",
    "        self.apply_rotary_emb = staticmethod(apply_rotary_emb)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.dummy.device\n",
    "\n",
    "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
    "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
    "\n",
    "    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        assert not self.use_xpos or exists(scale), 'you must use `.rotate_queries_and_keys` method instead and pass in both queries and keys, for length extrapolatable rotary embeddings'\n",
    "\n",
    "        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n",
    "\n",
    "        seq = self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset)\n",
    "\n",
    "        freqs = self.forward(seq, seq_len = seq_len, offset = offset)\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "\n",
    "        return apply_rotary_emb(freqs, t, scale = default(scale, 1.), seq_dim = seq_dim)\n",
    "\n",
    "    def rotate_queries_with_cached_keys(self, q, k, seq_dim = None, offset = 0):\n",
    "        dtype, device, seq_dim = q.dtype, q.device, default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        q_len, k_len = q.shape[seq_dim], k.shape[seq_dim]\n",
    "        assert q_len <= k_len\n",
    "\n",
    "        q_scale = k_scale = 1.\n",
    "\n",
    "        if self.use_xpos:\n",
    "            seq = self.get_seq_pos(k_len, dtype = dtype, device = device)\n",
    "\n",
    "            q_scale = self.get_scale(seq[-q_len:]).type(dtype)\n",
    "            k_scale = self.get_scale(seq).type(dtype)\n",
    "\n",
    "        rotated_q = self.rotate_queries_or_keys(q, seq_dim = seq_dim, scale = q_scale, offset = k_len - q_len + offset)\n",
    "        rotated_k = self.rotate_queries_or_keys(k, seq_dim = seq_dim, scale = k_scale ** -1)\n",
    "\n",
    "        rotated_q = rotated_q.type(q.dtype)\n",
    "        rotated_k = rotated_k.type(k.dtype)\n",
    "\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def rotate_queries_and_keys(self, q, k, seq_dim = None):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        assert self.use_xpos\n",
    "        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n",
    "\n",
    "        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n",
    "\n",
    "        freqs = self.forward(seq, seq_len = seq_len)\n",
    "        scale = self.get_scale(seq, seq_len = seq_len).to(dtype)\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "            scale = rearrange(scale, 'n d -> n 1 d')\n",
    "\n",
    "        rotated_q = apply_rotary_emb(freqs, q, scale = scale, seq_dim = seq_dim)\n",
    "        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1, seq_dim = seq_dim)\n",
    "\n",
    "        rotated_q = rotated_q.type(q.dtype)\n",
    "        rotated_k = rotated_k.type(k.dtype)\n",
    "\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def get_scale(\n",
    "        self,\n",
    "        t: Tensor,\n",
    "        seq_len: int | None = None,\n",
    "        offset = 0\n",
    "    ):\n",
    "        assert self.use_xpos\n",
    "\n",
    "        should_cache = (\n",
    "            self.cache_if_possible and\n",
    "            exists(seq_len) and\n",
    "            (offset + seq_len) <= self.cache_max_seq_len\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            should_cache and \\\n",
    "            exists(self.cached_scales) and \\\n",
    "            (seq_len + offset) <= self.cached_scales_seq_len.item()\n",
    "        ):\n",
    "            return self.cached_scales[offset:(offset + seq_len)]\n",
    "\n",
    "        scale = 1.\n",
    "        if self.use_xpos:\n",
    "            power = (t - len(t) // 2) / self.scale_base\n",
    "            scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "            scale = repeat(scale, 'n d -> n (d r)', r = 2)\n",
    "\n",
    "        if should_cache and offset == 0:\n",
    "            self.cached_scales[:seq_len] = scale.detach()\n",
    "            self.cached_scales_seq_len.copy_(seq_len)\n",
    "\n",
    "        return scale\n",
    "\n",
    "    def get_axial_freqs(self, *dims):\n",
    "        Colon = slice(None)\n",
    "        all_freqs = []\n",
    "\n",
    "        for ind, dim in enumerate(dims):\n",
    "            if self.freqs_for == 'pixel':\n",
    "                pos = torch.linspace(-1, 1, steps = dim, device = self.device)\n",
    "            else:\n",
    "                pos = torch.arange(dim, device = self.device)\n",
    "\n",
    "            freqs = self.forward(pos, seq_len = dim)\n",
    "\n",
    "            all_axis = [None] * len(dims)\n",
    "            all_axis[ind] = Colon\n",
    "\n",
    "            new_axis_slice = (Ellipsis, *all_axis, Colon)\n",
    "            all_freqs.append(freqs[new_axis_slice])\n",
    "\n",
    "        all_freqs = broadcast_tensors(*all_freqs)\n",
    "        return torch.cat(all_freqs, dim = -1)\n",
    "\n",
    "    @autocast('cuda', enabled = False)\n",
    "    def forward(\n",
    "        self,\n",
    "        t: Tensor,\n",
    "        seq_len = None,\n",
    "        offset = 0\n",
    "    ):\n",
    "        should_cache = (\n",
    "            self.cache_if_possible and\n",
    "            not self.learned_freq and\n",
    "            exists(seq_len) and\n",
    "            self.freqs_for != 'pixel' and\n",
    "            (offset + seq_len) <= self.cache_max_seq_len\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            should_cache and \\\n",
    "            exists(self.cached_freqs) and \\\n",
    "            (offset + seq_len) <= self.cached_freqs_seq_len.item()\n",
    "        ):\n",
    "            return self.cached_freqs[offset:(offset + seq_len)].detach()\n",
    "\n",
    "        freqs = self.freqs\n",
    "\n",
    "        freqs = einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n",
    "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
    "\n",
    "        if should_cache and offset == 0:\n",
    "            self.cached_freqs[:seq_len] = freqs.detach()\n",
    "            self.cached_freqs_seq_len.copy_(seq_len)\n",
    "\n",
    "        return freqs\n",
    "    \n",
    "# Inference code\n",
    "rotary_git4_emb = RotaryEmbedding(dim = q.size(3))\n",
    "rotary_git4_q = rotary_git4_emb.rotate_queries_or_keys(q)\n",
    "\n",
    "print(\"rotary_git4_q=\\n\", rotary_git4_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git4 - git3 rotary result :\n",
      " tensor(1.1921e-07)\n"
     ]
    }
   ],
   "source": [
    "diff_rotary_result = (rotary_git4_q - rotary_git3_q).max()\n",
    "print(\"git4 - git3 rotary result :\\n\", diff_rotary_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## git-5 LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 154\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_embed \u001b[38;5;66;03m#, k_embed\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m## inference code\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m rotary_emb \u001b[38;5;241m=\u001b[39m LlamaRotaryEmbedding(dim\u001b[38;5;241m=\u001b[39m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m,max_position_embeddings\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) )\n\u001b[1;32m    155\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m rotary_emb(q, torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, q\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m    156\u001b[0m query_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(q, cos, sin)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "def _compute_default_rope_parameters(\n",
    "    config  = None,\n",
    "    device= None,\n",
    "    seq_len: int = None,\n",
    "    **rope_kwargs,\n",
    ") :\n",
    "    \"\"\"\n",
    "    Computes the inverse frequencies according to the original RoPE implementation\n",
    "    Args:\n",
    "        config ([`~transformers.PretrainedConfig`]):\n",
    "            The model configuration.\n",
    "        device (`torch.device`):\n",
    "            The device to use for initialization of the inverse frequencies.\n",
    "        seq_len (`int`, *optional*):\n",
    "            The current sequence length. Unused for this type of RoPE.\n",
    "        rope_kwargs (`Dict`, *optional*):\n",
    "            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n",
    "    Returns:\n",
    "        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n",
    "        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n",
    "    \"\"\"\n",
    "    if config is not None and len(rope_kwargs) > 0:\n",
    "        raise ValueError(\n",
    "            \"Unexpected arguments: `**rope_kwargs` and `config` are mutually exclusive in \"\n",
    "            f\"`_compute_default_rope_parameters`, got `rope_kwargs`={rope_kwargs} and `config`={config}\"\n",
    "        )\n",
    "    if len(rope_kwargs) > 0:\n",
    "        base = rope_kwargs[\"base\"]\n",
    "        dim = rope_kwargs[\"dim\"]\n",
    "    elif config is not None:\n",
    "        base = config.rope_theta\n",
    "        partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n",
    "        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        dim = int(head_dim * partial_rotary_factor)\n",
    "\n",
    "    attention_factor = 1.0  # Unused in this type of RoPE\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "    return inv_freq, attention_factor\n",
    "\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=None,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000,\n",
    "        device=None,\n",
    "        scaling_factor=1.0,\n",
    "        rope_type=\"default\",\n",
    "        config = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO (joao): remove the `if` below, only used for BC\n",
    "        self.rope_kwargs = {}\n",
    "        if config is None:\n",
    "           \n",
    "            self.rope_kwargs = {\n",
    "                \"rope_type\": rope_type,\n",
    "                \"factor\": scaling_factor,\n",
    "                \"dim\": dim,\n",
    "                \"base\": base,\n",
    "                \"max_position_embeddings\": max_position_embeddings,\n",
    "            }\n",
    "            self.rope_type = rope_type\n",
    "            self.max_seq_len_cached = max_position_embeddings\n",
    "            self.original_max_seq_len = max_position_embeddings\n",
    "        else:\n",
    "            # BC: \"rope_type\" was originally \"type\"\n",
    "            if config.rope_scaling is not None:\n",
    "                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "            else:\n",
    "                self.rope_type = \"default\"\n",
    "            self.max_seq_len_cached = config.max_position_embeddings\n",
    "            self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_init_fn = _compute_default_rope_parameters\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    def _dynamic_frequency_update(self, position_ids, device):\n",
    "        \"\"\"\n",
    "        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n",
    "        1 - growing beyond the cached sequence length (allow scaling)\n",
    "        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n",
    "        \"\"\"\n",
    "        seq_len = torch.max(position_ids) + 1\n",
    "        if seq_len > self.max_seq_len_cached:  # growth\n",
    "            inv_freq, self.attention_scaling = self.rope_init_fn(\n",
    "                self.config, device, seq_len=seq_len, **self.rope_kwargs\n",
    "            )\n",
    "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n",
    "            self.max_seq_len_cached = seq_len\n",
    "\n",
    "        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n",
    "            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n",
    "            self.max_seq_len_cached = self.original_max_seq_len\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        if \"dynamic\" in self.rope_type:\n",
    "            self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "    \n",
    "def apply_rotary_pos_emb(q, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    # k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed #, k_embed\n",
    "\n",
    "\n",
    "## inference code\n",
    "rotary_emb = LlamaRotaryEmbedding(dim=q.size(3),max_position_embeddings=q.size(2) )\n",
    "cos, sin = rotary_emb(q, torch.arange(0, q.size(2)))\n",
    "query_states = apply_rotary_pos_emb(q, cos, sin)\n",
    "\n",
    "print(\"LLama rotary =\\n\", query_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## git-6 Diffusers and Mession\n",
    "\n",
    "implement rope-1d, rope-2d \n",
    "\n",
    "Flux only use rope-1d\n",
    "Lumina and HuanyuanDit  use rope-2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mession - git3 rotary result :\n",
      " tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Copied from Diffusers: https://github.com/huggingface/diffusers/blob/074e12358bc17e7dbe111ea4f62f05dbae8a49d5/src/diffusers/models/embeddings.py#L713C1-L777C1\n",
    "# Copied from Messionic: https://github.com/viiika/Meissonic/blob/0d9a5481292b1592acc2925c0b90726fa64fe8a5/src/transformer.py#L194C1-L248C25\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "def get_2d_rotary_pos_embed(embed_dim, crops_coords, grid_size, use_real=True):\n",
    "    \"\"\"\n",
    "    RoPE for image tokens with 2d structure.\n",
    "\n",
    "    Args:\n",
    "    embed_dim: (`int`):\n",
    "        The embedding dimension size\n",
    "    crops_coords (`Tuple[int]`)\n",
    "        The top-left and bottom-right coordinates of the crop.\n",
    "    grid_size (`Tuple[int]`):\n",
    "        The grid size of the positional embedding.\n",
    "    use_real (`bool`):\n",
    "        If True, return real part and imaginary part separately. Otherwise, return complex numbers.\n",
    "\n",
    "    Returns:\n",
    "        `torch.Tensor`: positional embedding with shape `( grid_size * grid_size, embed_dim/2)`.\n",
    "    \"\"\"\n",
    "    start, stop = crops_coords\n",
    "    grid_h = np.linspace(start[0], stop[0], grid_size[0], endpoint=False, dtype=np.float32)\n",
    "    grid_w = np.linspace(start[1], stop[1], grid_size[1], endpoint=False, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)  # [2, W, H]\n",
    "\n",
    "    grid = grid.reshape([2, 1, *grid.shape[1:]])\n",
    "    pos_embed = get_2d_rotary_pos_embed_from_grid(embed_dim, grid, use_real=use_real)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_rotary_pos_embed_from_grid(embed_dim, grid, use_real=False):\n",
    "    assert embed_dim % 4 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_rotary_pos_embed(\n",
    "        embed_dim // 2, grid[0].reshape(-1), use_real=use_real\n",
    "    )  # (H*W, D/2) if use_real else (H*W, D/4)\n",
    "    emb_w = get_1d_rotary_pos_embed(\n",
    "        embed_dim // 2, grid[1].reshape(-1), use_real=use_real\n",
    "    )  # (H*W, D/2) if use_real else (H*W, D/4)\n",
    "\n",
    "    if use_real:\n",
    "        cos = torch.cat([emb_h[0], emb_w[0]], dim=1)  # (H*W, D)\n",
    "        sin = torch.cat([emb_h[1], emb_w[1]], dim=1)  # (H*W, D)\n",
    "        return cos, sin\n",
    "    else:\n",
    "        emb = torch.cat([emb_h, emb_w], dim=1)  # (H*W, D/2)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def get_1d_rotary_pos_embed(\n",
    "    dim: int,\n",
    "    pos: Union[np.ndarray, int],\n",
    "    theta: float = 10000.0,\n",
    "    use_real=False,\n",
    "    linear_factor=1.0,\n",
    "    ntk_factor=1.0,\n",
    "    repeat_interleave_real=True,\n",
    "    freqs_dtype=torch.float32,  #  torch.float32, torch.float64 (flux)\n",
    "):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "\n",
    "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim' and the end\n",
    "    index 'end'. The 'theta' parameter scales the frequencies. The returned tensor contains complex values in complex64\n",
    "    data type.\n",
    "\n",
    "    Args:\n",
    "        dim (`int`): Dimension of the frequency tensor.\n",
    "        pos (`np.ndarray` or `int`): Position indices for the frequency tensor. [S] or scalar\n",
    "        theta (`float`, *optional*, defaults to 10000.0):\n",
    "            Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        use_real (`bool`, *optional*):\n",
    "            If True, return real part and imaginary part separately. Otherwise, return complex numbers.\n",
    "        linear_factor (`float`, *optional*, defaults to 1.0):\n",
    "            Scaling factor for the context extrapolation. Defaults to 1.0.\n",
    "        ntk_factor (`float`, *optional*, defaults to 1.0):\n",
    "            Scaling factor for the NTK-Aware RoPE. Defaults to 1.0.\n",
    "        repeat_interleave_real (`bool`, *optional*, defaults to `True`):\n",
    "            If `True` and `use_real`, real part and imaginary part are each interleaved with themselves to reach `dim`.\n",
    "            Otherwise, they are concateanted with themselves.\n",
    "        freqs_dtype (`torch.float32` or `torch.float64`, *optional*, defaults to `torch.float32`):\n",
    "            the dtype of the frequency tensor.\n",
    "    Returns:\n",
    "        `torch.Tensor`: Precomputed frequency tensor with complex exponentials. [S, D/2]\n",
    "    \"\"\"\n",
    "    assert dim % 2 == 0\n",
    "\n",
    "    if isinstance(pos, int):\n",
    "        pos = torch.arange(pos)\n",
    "    if isinstance(pos, np.ndarray):\n",
    "        pos = torch.from_numpy(pos)  # type: ignore  # [S]\n",
    "\n",
    "    theta = theta * ntk_factor\n",
    "    freqs = (\n",
    "        1.0\n",
    "        / (theta ** (torch.arange(0, dim, 2, dtype=freqs_dtype, device=pos.device)[: (dim // 2)] / dim))\n",
    "        / linear_factor\n",
    "    )  # [D/2]\n",
    "    freqs = torch.outer(pos, freqs)  # type: ignore   # [S, D/2]\n",
    "    if use_real and repeat_interleave_real:\n",
    "        # flux, hunyuan-dit, cogvideox\n",
    "        freqs_cos = freqs.cos().repeat_interleave(2, dim=1).float()  # [S, D]\n",
    "        freqs_sin = freqs.sin().repeat_interleave(2, dim=1).float()  # [S, D]\n",
    "        return freqs_cos, freqs_sin\n",
    "    elif use_real:\n",
    "        # stable audio, allegro\n",
    "        freqs_cos = torch.cat([freqs.cos(), freqs.cos()], dim=-1).float()  # [S, D]\n",
    "        freqs_sin = torch.cat([freqs.sin(), freqs.sin()], dim=-1).float()  # [S, D]\n",
    "        return freqs_cos, freqs_sin\n",
    "    else:\n",
    "        # lumina\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64     # [S, D/2]\n",
    "        return freqs_cis\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    freqs_cis: Union[torch.Tensor, Tuple[torch.Tensor]],\n",
    "    use_real: bool = True,\n",
    "    use_real_unbind_dim: int = -1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor. This function applies rotary embeddings\n",
    "    to the given query or key 'x' tensors using the provided frequency tensor 'freqs_cis'. The input tensors are\n",
    "    reshaped as complex numbers, and the frequency tensor is reshaped for broadcasting compatibility. The resulting\n",
    "    tensors contain rotary embeddings and are returned as real tensors.\n",
    "\n",
    "    Args:\n",
    "        x (`torch.Tensor`):\n",
    "            Query or key tensor to apply rotary embeddings. [B, H, S, D] xk (torch.Tensor): Key tensor to apply\n",
    "        freqs_cis (`Tuple[torch.Tensor]`): Precomputed frequency tensor for complex exponentials. ([S, D], [S, D],)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
    "    \"\"\"\n",
    "    if use_real:\n",
    "        cos, sin = freqs_cis  # [S, D]\n",
    "        n_squeeze = x.ndim-2\n",
    "        for i in range(n_squeeze):\n",
    "            cos = cos.unsqueeze(0)\n",
    "            sin = sin.unsqueeze(0)\n",
    "        cos, sin = cos.to(x.device), sin.to(x.device)\n",
    "\n",
    "        if use_real_unbind_dim == -1:\n",
    "            # Used for flux, cogvideox, hunyuan-dit\n",
    "            x_real, x_imag = x.reshape(*x.shape[:-1], -1, 2).unbind(-1)  # [B, S, H, D//2]\n",
    "            x_rotated = torch.stack([-x_imag, x_real], dim=-1).flatten(-2)  # -2 is last dimension\n",
    "        elif use_real_unbind_dim == -2:\n",
    "            # Used for Stable Audio\n",
    "            x_real, x_imag = x.reshape(*x.shape[:-1], 2, -1).unbind(-2)  # [B, S, H, D//2]\n",
    "            x_rotated = torch.cat([-x_imag, x_real], dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"`use_real_unbind_dim={use_real_unbind_dim}` but should be -1 or -2.\")\n",
    "\n",
    "        out = (x.float() * cos + x_rotated.float() * sin).to(x.dtype)\n",
    "\n",
    "        return out\n",
    "    else:\n",
    "        # used for lumina\n",
    "        x_rotated = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "        freqs_cis = freqs_cis.unsqueeze(2)\n",
    "        x_out = torch.view_as_real(x_rotated * freqs_cis).flatten(3)\n",
    "\n",
    "        return x_out.type_as(x)\n",
    "    \n",
    "\n",
    "# Inference code \n",
    "image_rotary_emb = get_1d_rotary_pos_embed(\n",
    "    dim=q.size(-1), \n",
    "    pos=q.size(-2),     # or [np.array([0,1, 2, 3, 4])]\n",
    "    repeat_interleave_real=True, use_real=True, freqs_dtype=torch.float64\n",
    ") # cos, sin\n",
    "\n",
    "mession_q = apply_rotary_emb(q, image_rotary_emb)\n",
    "\n",
    "diff_mession_result = (mession_q - rotary_git3_q).max()\n",
    "print(\"mession - git3 rotary result :\\n\", diff_mession_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D rope in VisionTransformer\n",
    "reference : hunyuanDit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n",
      "x_rotary_size: torch.Size([2, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "grid_size = 3\n",
    "seq_dim = 20\n",
    "\n",
    "cls_feature = torch.randn((2, 1, seq_dim))      # (bs, seq=1, seq_dim)\n",
    "img_feature = torch.randn((2, grid_size*grid_size, seq_dim))   # (bs, seq=3*3,  seq_dim)\n",
    "\n",
    "cls_rotary2d_emb = get_2d_rotary_pos_embed(\n",
    "    cls_feature.size(-1), \n",
    "    ((0,0), (1, 1)), \n",
    "    grid_size=(1,1), use_real=True)\n",
    "image_rotary2d_emb = get_2d_rotary_pos_embed(\n",
    "    img_feature.size(-1), \n",
    "    ((1,1),(1+grid_size, 1+grid_size)), \n",
    "    grid_size=(grid_size,grid_size),use_real=True)\n",
    "\n",
    "image_rotary_emb = (\n",
    "    torch.cat([cls_rotary2d_emb[0], image_rotary2d_emb[0]], dim=0),\n",
    "    torch.cat([cls_rotary2d_emb[1], image_rotary2d_emb[1]], dim=0),\n",
    ")\n",
    "print(image_rotary_emb[0].size())\n",
    "\n",
    "\n",
    "x_cat = torch.cat([cls_feature, img_feature], dim=1)\n",
    "\n",
    "x_rotary = apply_rotary_emb(x_cat, image_rotary_emb)\n",
    "\n",
    "print(\"x_rotary_size:\", x_rotary.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## git-6 Flux Rope Implement\n",
    "Copied from https://github.com/black-forest-labs/flux/blob/main/src/flux/math.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def attention(q: Tensor, k: Tensor, v: Tensor, pe: Tensor) -> Tensor:\n",
    "    q, k = apply_rope(q, k, pe)\n",
    "\n",
    "    x = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "    x = rearrange(x, \"B H L D -> B L (H D)\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def rope(pos: Tensor, dim: int, theta: int) -> Tensor:\n",
    "    assert dim % 2 == 0\n",
    "    scale = torch.arange(0, dim, 2, dtype=torch.float64, device=pos.device) / dim\n",
    "    omega = 1.0 / (theta**scale)\n",
    "    out = torch.einsum(\"...n,d->...nd\", pos, omega)\n",
    "    out = torch.stack([torch.cos(out), -torch.sin(out), torch.sin(out), torch.cos(out)], dim=-1)\n",
    "    out = rearrange(out, \"b n d (i j) -> b n d i j\", i=2, j=2)\n",
    "    return out.float()\n",
    "\n",
    "\n",
    "def apply_rope(xq: Tensor, xk: Tensor, freqs_cis: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 1, 2)\n",
    "    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 1, 2)\n",
    "    xq_out = freqs_cis[..., 0] * xq_[..., 0] + freqs_cis[..., 1] * xq_[..., 1]\n",
    "    xk_out = freqs_cis[..., 0] * xk_[..., 0] + freqs_cis[..., 1] * xk_[..., 1]\n",
    "    return xq_out.reshape(*xq.shape).type_as(xq), xk_out.reshape(*xk.shape).type_as(xk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "font",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
